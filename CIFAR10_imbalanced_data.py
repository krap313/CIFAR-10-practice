"""
CIFAR10의 데이터가가 불균형(imbalanced data)인 상황을 가정하고 분류 모델을 학습

<불균형 데이터로 분류 모델링하는 방법>
1. 소수 클래스의 데이터 추가 수집(데이터 증강)
2. 불균형 데이터 분류 모델에 적합한 성능평가 지표 선정
3. 샘플링 방법(Undersampling, Oversampling)
4. 비용 또는 가중치 조정 방법
5. Outlier detection 방법
6. 확률 튜닝 알고리즘


<Under Sampling>
- 다수 범주의 데이터를 소수 범주 데이터 수에 맞게 줄이는 샘플링 방식
- 다수 범주 데이터의 제거로 계산 시간 감소
- 데이터 제거로 인한 정보 손실이 발생
1. Random Sampling
- 다수 범주에서 무작위로 샘플링
- 할 때마다 다른 결괄르 얻는다는 단점

2. Tomek Links
- 두 범주 사이를 탐지하고 정리를 통해 부정확한 분류경계선을 방지
- Tomek Link를 찾고 다수 범주에 속한 관측치를 제거하여 언더 샘플링을 한다.
- Tomek Link : 선택한 두 데이터 xk까지의 거리보다 선택한 두 데이터 사이의 
거리가 짧을 때 선택한 두 데이터 간의 링크(두 데이터를 다른 클래스)

3. CNN Rule
- ?

4. One Sided Selection
- Tomek Links와 CNN Rule을 합친 방법



<Over Sampling>
- 데이터를 증가시키기 때문에 정보 손실이 없다
- 대부분의 경우 언더 샘플링에 비해 높은 정확도
- 데이터 증가로 인해 계산 시간이 증가, 과적합 가능성 존재
- 노이즈, 이상치에 민감
1. Resampling
- 소수 범주의 데이터 수를 다수 범주의 데이터 수와 비슷해지도록 증가
- 소수 범주의 데이터는 무작위로 복제
- 소수 범주에 과적합이 발생할 수 있다

2. SMOTE(Resampling의 과적합 보완)
- 소수 범주에서 가상의 데이터를 생성
- K값을 정한 후 소수 범주에서 임의의 데이터를 선택한 후 선택한 데이터와 가장
가까운 K개의 데이터 중 하나를 무작위로 선정해 Synthetic 공식을 통해 가상의 데이터 생성
- 이 과정을 소수 범주에 속하는 모든 데이터에 대해 수행
- K값은 2 이상
- Synthetic 공식?

3. Borderline SMOTE
- Borderline 부분에 대해서만 SMOTE 방식 사용
- 임의의 소수 범주의 데이터 한 개에 대해서 주변의 K개 데이터를 탐색하고 그 중
다수 범주 데이터의 수를 확인하여 Noise 관측치, Danger 관측치, Sage 관측치로
나누어 Dager 관측치에 대해서만 SMOTE를 적용
- 다수 범주 데이터의 수 = K => Noise
- K/2 < 다수 범주 데이터의 수 < K => Dager
- 다수 범주 데이터의 수 < K/2 => Safe

4. ADASYN
- Borderline SMOTE와 비슷하지만 샘플링 개수를 데이터 위치에 따라 다르게 설정
- 모든 소수 범주 데이터에 대해 주변의 K개의 데이터를 탐색 후 다수 범주 데이터의 비율을
계산하여 각 비율들을 비율의 총합으로 나눠 스케일링을 진행
- 스케일링이 진행된 비율에 (다수 범주 데이터 수 - 소수 범주 데이터 수) 곱해주고
반올림 된 정수의 값만큼 각 소수 범주 데이터 주변에 SMOTE 방식을 가상 데이터 생성
- 소수 범주 데이터 주변의 다수 범주 데이터의 수에 따라 유동적으로 생성이 가능


5. GAN(Generative Adversarial Nets)
- 생성자와 구분자로 구성
- 무작위로 노이즈를 생성하고 생성자를 통해 가짜 샘플을 만든 후 구분자에서
진짜 샘플과 가짜 샘플을 판별하고 너무 쉽게 판별될 경우 생성자에세 피드백
생성자는 더욱 진짜 샘플과 비슷한 가짜 샘플을 만들어내고 구분자에게 판별 시킴
- 이처럼 생성자와 구분자가 서로 경쟁하며 업데이트 되고 가짜 샘플은 진짜 샘플과
유사한 형태로 생성
"""

